---
## OpenWebUI with Ollama Helm Chart Override Values
## Enhanced with detailed logging and resource configurations

ollama:
  ollama:
    ## GPU configuration for Ollama
    gpu:
      enabled: true
      type: 'nvidia'
      number: 1
    ## Models to pull on startup
    ## More models you pull, more time it takes to start
    ## You can also pull models later using the ollama CLI
    ## See https://ollama.com/docs/models for more models
    models:
      pull:
        - qwen3:14b
        - llama3.1:8b
        - gemma3:12b
        - deepseek-r1:14b
    ## Resource limits for Ollama
    resources:
      requests:
        cpu: "1000m"
        memory: "4Gi"
        nvidia.com/gpu: 1
      limits:
        cpu: "4000m"
        memory: "8Gi"
        nvidia.com/gpu: 1
    ## Enable verbose logging
    env:
      - name: OLLAMA_DEBUG
        value: "1"
  ## Use NVIDIA runtime
  runtimeClassName: nvidia
  # Enable persistent storage
  persistentVolume:
    enabled: true
    size: "20Gi"
  ## Node affinity for GPU nodes
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.present
            operator: In
            values:
            - "true"
          - key: nvidia.com/gpu.count
            operator: Gt
            values:
            - "0"
## Enable Apache Tika for document processing
tika:
  enabled: true
  resources:
    requests:
      cpu: "200m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "1Gi"

## Enable WebSocket support
websocket:
  enabled: true

## Health check configuration
livenessProbe:
  httpGet:
    path: /health
    port: http
  failureThreshold: 1
  periodSeconds: 10
  timeoutSeconds: 5

readinessProbe:
  httpGet:
    path: /health/db
    port: http
  failureThreshold: 1
  periodSeconds: 10
  timeoutSeconds: 5

startupProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 30
  periodSeconds: 5
  failureThreshold: 20
  timeoutSeconds: 5
ingress:
  enabled: true
  class: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "http"

    ## for LLMs with long response times
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600" ##how long to wait for backend to send data
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600" ##how long to wait while sending data to client
    nginx.ingress.kubernetes.io/proxy-body-size: "128m" ##max body size
    nginx.ingress.kubernetes.io/proxy-buffering: "off" ##disable buffering for streaming responses
    
    ## For WebSocket support without snippets
    nginx.ingress.kubernetes.io/websocket-services: "open-webui" ##enable websocket support
    nginx.ingress.kubernetes.io/upstream-hash-by: "$binary_remote_addr" ##determine which backend to use based on client IP
    nginx.ingress.kubernetes.io/affinity: "cookie" ##enable sticky sessions
    nginx.ingress.kubernetes.io/session-cookie-name: "route" ##name of the session
    nginx.ingress.kubernetes.io/session-cookie-expires: "172800" ##cookie expiration time in seconds
    nginx.ingress.kubernetes.io/session-cookie-max-age: "172800" ##cookie max age in seconds
  host: myai01.abiwot-lab.com
  tls: true
  existingSecret: "imported-abiwot-lab-cert" ##name of the secret containing the TLS certificate
persistence:
  storageClass: "local-path-retain"

## -- Extra resources to deploy with OpenWebUI
extraResources:
  ## -- TLS secret for abiwot-lab.com
  ## -- This is a dummy certificate, you should replace it with your own certificate
  - apiVersion: v1
    kind: Secret
    metadata:
      name: imported-abiwot-lab-cert
      namespace: myai01
    type: kubernetes.io/tls
    data:
      tls.crt: <CHANGE ME - base64 encoded TLS certificate>
      tls.key: <CHANGE ME - base64 encoded TLS key>

  ## -- TLS certificate for abiwot-lab.com
  ## -- This is a dummy certificate, you should replace it with your own certificate
  - apiVersion: cert-manager.io/v1
    kind: Certificate
    metadata:
      name: imported-abiwot-lab-cert
      namespace: myai01
    spec:
      secretName: imported-abiwot-lab-cert
      commonName: abiwot-lab.com
      dnsNames:
        - '*.abiwot-lab.com'  ##If you have a wildcard certificate
      issuerRef:
        ## This is required but won't be used for renewals
        name: dummy-issuer
        kind: ClusterIssuer
        group: cert-manager.io
  
  ## -- NetworkPolicy to prevent multiple 'myaixx' instances from communicating with each other
  - apiVersion: crd.projectcalico.org/v1
    kind: GlobalNetworkPolicy
    metadata:
      name: block-myai-cross-namespace
    spec:
      selector: all()
      order: 500
      types:
      - Ingress
      - Egress

      # Block traffic between different myai namespaces for incoming connections
      ingress:
      - action: Deny
        source:
          selector: k8s.io/namespace =~ 'myai.*'
        destination:
          selector: k8s.io/namespace =~ 'myai.*' && k8s.io/namespace != '$$source.k8s.io/namespace'

      # Allow all other traffic by default
      - action: Pass

      # Block traffic between different myai namespaces for outgoing connections
      egress:
      - action: Deny
        source:
          selector: k8s.io/namespace =~ 'myai.*'
        destination:
          selector: k8s.io/namespace =~ 'myai.*' && k8s.io/namespace != '$$source.k8s.io/namespace'

      # Allow all other traffic by default
      - action: Pass

## -- Configure database URL, needed to work with Postgres
## syntax format: postgresql://<username>:<password>@<host>:<port>/<database>
databaseUrl: postgresql://open-webui:0p3n-w3bu!@open-webui-postgres:5432/open-webui

## -- Postgresql configuration (see. https://artifacthub.io/packages/helm/bitnami/postgresql)
postgresql:
  enabled: true
  
  ## Recommend to use postgresql.existingSecret & postgresql.secretKeys to keep the password secret
  auth:
    database: open-webui
    postgresPassword: 0p3n-w3bu!
    username: open-webui
    password: 0p3n-w3bu!
  
  ## Configure PostgreSQL with proper resources and logging
  primary:
    storageClass: "local-path-retain"
    persistence:
      size: "10Gi"
    resources:
      requests:
        cpu: "200m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "1Gi"
...